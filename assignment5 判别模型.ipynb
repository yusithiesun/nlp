{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、使用最大熵分类器实现人名分类。并与朴素贝叶斯分类器进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.37529        0.762\n",
      "             3          -0.37494        0.762\n",
      "             4          -0.37473        0.762\n",
      "             5          -0.37459        0.762\n",
      "             6          -0.37449        0.762\n",
      "             7          -0.37441        0.762\n",
      "             8          -0.37436        0.762\n",
      "             9          -0.37431        0.762\n",
      "            10          -0.37427        0.762\n",
      "            11          -0.37424        0.762\n",
      "            12          -0.37421        0.762\n",
      "            13          -0.37419        0.762\n",
      "            14          -0.37417        0.762\n",
      "            15          -0.37415        0.762\n",
      "            16          -0.37414        0.762\n",
      "            17          -0.37412        0.762\n",
      "            18          -0.37411        0.762\n",
      "            19          -0.37410        0.762\n",
      "            20          -0.37409        0.762\n",
      "            21          -0.37408        0.762\n",
      "            22          -0.37407        0.762\n",
      "            23          -0.37407        0.762\n",
      "            24          -0.37406        0.762\n",
      "            25          -0.37405        0.762\n",
      "            26          -0.37405        0.762\n",
      "            27          -0.37404        0.762\n",
      "            28          -0.37404        0.762\n",
      "            29          -0.37403        0.762\n",
      "            30          -0.37403        0.762\n",
      "            31          -0.37402        0.762\n",
      "            32          -0.37402        0.762\n",
      "            33          -0.37401        0.762\n",
      "            34          -0.37401        0.762\n",
      "            35          -0.37401        0.762\n",
      "            36          -0.37400        0.762\n",
      "            37          -0.37400        0.762\n",
      "            38          -0.37400        0.762\n",
      "            39          -0.37400        0.762\n",
      "            40          -0.37399        0.762\n",
      "            41          -0.37399        0.762\n",
      "            42          -0.37399        0.762\n",
      "            43          -0.37399        0.762\n",
      "            44          -0.37398        0.762\n",
      "            45          -0.37398        0.762\n",
      "            46          -0.37398        0.762\n",
      "            47          -0.37398        0.762\n",
      "            48          -0.37398        0.762\n",
      "            49          -0.37397        0.762\n",
      "            50          -0.37397        0.762\n",
      "            51          -0.37397        0.762\n",
      "            52          -0.37397        0.762\n",
      "            53          -0.37397        0.762\n",
      "            54          -0.37397        0.762\n",
      "            55          -0.37397        0.762\n",
      "            56          -0.37396        0.762\n",
      "            57          -0.37396        0.762\n",
      "            58          -0.37396        0.762\n",
      "            59          -0.37396        0.762\n",
      "            60          -0.37396        0.762\n",
      "            61          -0.37396        0.762\n",
      "            62          -0.37396        0.762\n",
      "            63          -0.37396        0.762\n",
      "            64          -0.37396        0.762\n",
      "            65          -0.37395        0.762\n",
      "            66          -0.37395        0.762\n",
      "            67          -0.37395        0.762\n",
      "            68          -0.37395        0.762\n",
      "            69          -0.37395        0.762\n",
      "            70          -0.37395        0.762\n",
      "            71          -0.37395        0.762\n",
      "            72          -0.37395        0.762\n",
      "            73          -0.37395        0.762\n",
      "            74          -0.37395        0.762\n",
      "            75          -0.37395        0.762\n",
      "            76          -0.37395        0.762\n",
      "            77          -0.37394        0.762\n",
      "            78          -0.37394        0.762\n",
      "            79          -0.37394        0.762\n",
      "            80          -0.37394        0.762\n",
      "            81          -0.37394        0.762\n",
      "            82          -0.37394        0.762\n",
      "            83          -0.37394        0.762\n",
      "            84          -0.37394        0.762\n",
      "            85          -0.37394        0.762\n",
      "            86          -0.37394        0.762\n",
      "            87          -0.37394        0.762\n",
      "            88          -0.37394        0.762\n",
      "            89          -0.37394        0.762\n",
      "            90          -0.37394        0.762\n",
      "            91          -0.37394        0.762\n",
      "            92          -0.37394        0.762\n",
      "            93          -0.37394        0.762\n",
      "            94          -0.37393        0.762\n",
      "            95          -0.37393        0.762\n",
      "            96          -0.37393        0.762\n",
      "            97          -0.37393        0.762\n",
      "            98          -0.37393        0.762\n",
      "            99          -0.37393        0.762\n",
      "         Final          -0.37393        0.762\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "#classifier = nltk.MaxentClassifier.train(train_set)\n",
    "def gender_features(word):\n",
    "    return {'last_letter':word[-1]}\n",
    "#从语料库中取出需要的名字-性别数据，并进行预处理\n",
    "from nltk.corpus import names\n",
    "labeled_names=([(name,'male') for name in names.words('male.txt')]+\n",
    "               [(name,'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "import nltk\n",
    "featuresets=[(gender_features(n),gender) for (n,gender) in labeled_names]\n",
    "train_set=featuresets[1000:]\n",
    "test_set=featuresets[:1000]\n",
    "classifier = nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier,test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、基于NLTK实现影评NaiveBayes分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories()\n",
    "for fileid in movie_reviews.fileids(category)]  #转化为词列表的影评，与标签，组成二元组\n",
    "import random\n",
    "random.shuffle(documents)\t\t#为组成训练集和测试集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "all_words=nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features=list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words=set(document)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)]=(word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets=[(document_features(d),c) for (d,c) in documents]\n",
    "train_set,test_set=featuresets[100:],featuresets[:100]\n",
    "classifier=nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier,test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、基于最大熵模型实现影评分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python3.6\\lib\\site-packages\\nltk\\classify\\maxent.py:1386: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "d:\\python3.6\\lib\\site-packages\\nltk\\classify\\maxent.py:1388: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "d:\\python3.6\\lib\\site-packages\\nltk\\classify\\maxent.py:1389: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n",
      "d:\\python3.6\\lib\\site-packages\\nltk\\classify\\maxent.py:1396: RuntimeWarning: invalid value encountered in true_divide\n",
      "  deltas -= (ffreq_empirical - sum1) / -sum2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "all_words=nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features=list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words=set(document)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)]=(word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets=[(document_features(d),c) for (d,c) in documents]\n",
    "train_set,test_set=featuresets[100:],featuresets[:100]\n",
    "classifier=nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier,test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验10 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、基于LDA分析题目-词矩阵，并与SVD结果比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 395\n",
      "INFO:lda:vocab_size: 4258\n",
      "INFO:lda:n_words: 84010\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1051748\n",
      "INFO:lda:<10> log likelihood: -719800\n",
      "INFO:lda:<20> log likelihood: -699115\n",
      "INFO:lda:<30> log likelihood: -689370\n",
      "INFO:lda:<40> log likelihood: -684918\n",
      "INFO:lda:<50> log likelihood: -681322\n",
      "INFO:lda:<60> log likelihood: -678979\n",
      "INFO:lda:<70> log likelihood: -676598\n",
      "INFO:lda:<80> log likelihood: -675383\n",
      "INFO:lda:<90> log likelihood: -673316\n",
      "INFO:lda:<100> log likelihood: -672761\n",
      "INFO:lda:<110> log likelihood: -671320\n",
      "INFO:lda:<120> log likelihood: -669744\n",
      "INFO:lda:<130> log likelihood: -669292\n",
      "INFO:lda:<140> log likelihood: -667940\n",
      "INFO:lda:<150> log likelihood: -668038\n",
      "INFO:lda:<160> log likelihood: -667429\n",
      "INFO:lda:<170> log likelihood: -666475\n",
      "INFO:lda:<180> log likelihood: -665562\n",
      "INFO:lda:<190> log likelihood: -664920\n",
      "INFO:lda:<200> log likelihood: -664979\n",
      "INFO:lda:<210> log likelihood: -664722\n",
      "INFO:lda:<220> log likelihood: -664459\n",
      "INFO:lda:<230> log likelihood: -664360\n",
      "INFO:lda:<240> log likelihood: -663600\n",
      "INFO:lda:<250> log likelihood: -664164\n",
      "INFO:lda:<260> log likelihood: -663826\n",
      "INFO:lda:<270> log likelihood: -663458\n",
      "INFO:lda:<280> log likelihood: -663393\n",
      "INFO:lda:<290> log likelihood: -662904\n",
      "INFO:lda:<300> log likelihood: -662294\n",
      "INFO:lda:<310> log likelihood: -662031\n",
      "INFO:lda:<320> log likelihood: -662430\n",
      "INFO:lda:<330> log likelihood: -661601\n",
      "INFO:lda:<340> log likelihood: -662108\n",
      "INFO:lda:<350> log likelihood: -662152\n",
      "INFO:lda:<360> log likelihood: -661899\n",
      "INFO:lda:<370> log likelihood: -661012\n",
      "INFO:lda:<380> log likelihood: -661278\n",
      "INFO:lda:<390> log likelihood: -661085\n",
      "INFO:lda:<400> log likelihood: -660418\n",
      "INFO:lda:<410> log likelihood: -660510\n",
      "INFO:lda:<420> log likelihood: -660343\n",
      "INFO:lda:<430> log likelihood: -659789\n",
      "INFO:lda:<440> log likelihood: -659336\n",
      "INFO:lda:<450> log likelihood: -659039\n",
      "INFO:lda:<460> log likelihood: -659329\n",
      "INFO:lda:<470> log likelihood: -658707\n",
      "INFO:lda:<480> log likelihood: -658879\n",
      "INFO:lda:<490> log likelihood: -658819\n",
      "INFO:lda:<500> log likelihood: -658407\n",
      "INFO:lda:<510> log likelihood: -658651\n",
      "INFO:lda:<520> log likelihood: -658111\n",
      "INFO:lda:<530> log likelihood: -658018\n",
      "INFO:lda:<540> log likelihood: -658111\n",
      "INFO:lda:<550> log likelihood: -657925\n",
      "INFO:lda:<560> log likelihood: -657860\n",
      "INFO:lda:<570> log likelihood: -657494\n",
      "INFO:lda:<580> log likelihood: -657723\n",
      "INFO:lda:<590> log likelihood: -657591\n",
      "INFO:lda:<600> log likelihood: -657557\n",
      "INFO:lda:<610> log likelihood: -657505\n",
      "INFO:lda:<620> log likelihood: -657730\n",
      "INFO:lda:<630> log likelihood: -657304\n",
      "INFO:lda:<640> log likelihood: -657208\n",
      "INFO:lda:<650> log likelihood: -657518\n",
      "INFO:lda:<660> log likelihood: -657541\n",
      "INFO:lda:<670> log likelihood: -657381\n",
      "INFO:lda:<680> log likelihood: -657575\n",
      "INFO:lda:<690> log likelihood: -656985\n",
      "INFO:lda:<700> log likelihood: -656815\n",
      "INFO:lda:<710> log likelihood: -656930\n",
      "INFO:lda:<720> log likelihood: -656538\n",
      "INFO:lda:<730> log likelihood: -656291\n",
      "INFO:lda:<740> log likelihood: -656417\n",
      "INFO:lda:<750> log likelihood: -656747\n",
      "INFO:lda:<760> log likelihood: -656600\n",
      "INFO:lda:<770> log likelihood: -656269\n",
      "INFO:lda:<780> log likelihood: -656311\n",
      "INFO:lda:<790> log likelihood: -656069\n",
      "INFO:lda:<800> log likelihood: -656228\n",
      "INFO:lda:<810> log likelihood: -656178\n",
      "INFO:lda:<820> log likelihood: -655694\n",
      "INFO:lda:<830> log likelihood: -655997\n",
      "INFO:lda:<840> log likelihood: -656224\n",
      "INFO:lda:<850> log likelihood: -656197\n",
      "INFO:lda:<860> log likelihood: -655889\n",
      "INFO:lda:<870> log likelihood: -656180\n",
      "INFO:lda:<880> log likelihood: -656997\n",
      "INFO:lda:<890> log likelihood: -655989\n",
      "INFO:lda:<900> log likelihood: -655615\n",
      "INFO:lda:<910> log likelihood: -655584\n",
      "INFO:lda:<920> log likelihood: -656602\n",
      "INFO:lda:<930> log likelihood: -656083\n",
      "INFO:lda:<940> log likelihood: -656294\n",
      "INFO:lda:<950> log likelihood: -656257\n",
      "INFO:lda:<960> log likelihood: -656243\n",
      "INFO:lda:<970> log likelihood: -656028\n",
      "INFO:lda:<980> log likelihood: -655603\n",
      "INFO:lda:<990> log likelihood: -656012\n",
      "INFO:lda:<1000> log likelihood: -655849\n",
      "INFO:lda:<1010> log likelihood: -655376\n",
      "INFO:lda:<1020> log likelihood: -655417\n",
      "INFO:lda:<1030> log likelihood: -655856\n",
      "INFO:lda:<1040> log likelihood: -655197\n",
      "INFO:lda:<1050> log likelihood: -655938\n",
      "INFO:lda:<1060> log likelihood: -655529\n",
      "INFO:lda:<1070> log likelihood: -655092\n",
      "INFO:lda:<1080> log likelihood: -655119\n",
      "INFO:lda:<1090> log likelihood: -656215\n",
      "INFO:lda:<1100> log likelihood: -655602\n",
      "INFO:lda:<1110> log likelihood: -655296\n",
      "INFO:lda:<1120> log likelihood: -655547\n",
      "INFO:lda:<1130> log likelihood: -655580\n",
      "INFO:lda:<1140> log likelihood: -655604\n",
      "INFO:lda:<1150> log likelihood: -655168\n",
      "INFO:lda:<1160> log likelihood: -655281\n",
      "INFO:lda:<1170> log likelihood: -655409\n",
      "INFO:lda:<1180> log likelihood: -655517\n",
      "INFO:lda:<1190> log likelihood: -654922\n",
      "INFO:lda:<1200> log likelihood: -655304\n",
      "INFO:lda:<1210> log likelihood: -655852\n",
      "INFO:lda:<1220> log likelihood: -655184\n",
      "INFO:lda:<1230> log likelihood: -655650\n",
      "INFO:lda:<1240> log likelihood: -655606\n",
      "INFO:lda:<1250> log likelihood: -656086\n",
      "INFO:lda:<1260> log likelihood: -655698\n",
      "INFO:lda:<1270> log likelihood: -655351\n",
      "INFO:lda:<1280> log likelihood: -655686\n",
      "INFO:lda:<1290> log likelihood: -654801\n",
      "INFO:lda:<1300> log likelihood: -654973\n",
      "INFO:lda:<1310> log likelihood: -655186\n",
      "INFO:lda:<1320> log likelihood: -655128\n",
      "INFO:lda:<1330> log likelihood: -655365\n",
      "INFO:lda:<1340> log likelihood: -655338\n",
      "INFO:lda:<1350> log likelihood: -655219\n",
      "INFO:lda:<1360> log likelihood: -655115\n",
      "INFO:lda:<1370> log likelihood: -654930\n",
      "INFO:lda:<1380> log likelihood: -655209\n",
      "INFO:lda:<1390> log likelihood: -654940\n",
      "INFO:lda:<1400> log likelihood: -655055\n",
      "INFO:lda:<1410> log likelihood: -655286\n",
      "INFO:lda:<1420> log likelihood: -655316\n",
      "INFO:lda:<1430> log likelihood: -655257\n",
      "INFO:lda:<1440> log likelihood: -654964\n",
      "INFO:lda:<1450> log likelihood: -654884\n",
      "INFO:lda:<1460> log likelihood: -655493\n",
      "INFO:lda:<1470> log likelihood: -655415\n",
      "INFO:lda:<1480> log likelihood: -655192\n",
      "INFO:lda:<1490> log likelihood: -655728\n",
      "INFO:lda:<1499> log likelihood: -655858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: british churchill sale million major letters west\n",
      "Topic 1: church government political country state people party\n",
      "Topic 2: elvis king fans presley life concert young\n",
      "Topic 3: yeltsin russian russia president kremlin moscow michael\n",
      "Topic 4: pope vatican paul john surgery hospital pontiff\n",
      "Topic 5: family funeral police miami versace cunanan city\n",
      "Topic 6: simpson former years court president wife south\n",
      "Topic 7: order mother successor election nuns church nirmala\n",
      "Topic 8: charles prince diana royal king queen parker\n",
      "Topic 9: film french france against bardot paris poster\n",
      "Topic 10: germany german war nazi letter christian book\n",
      "Topic 11: east peace prize award timor quebec belo\n",
      "Topic 12: n't life show told very love television\n",
      "Topic 13: years year time last church world people\n",
      "Topic 14: mother teresa heart calcutta charity nun hospital\n",
      "Topic 15: city salonika capital buddhist cultural vietnam byzantine\n",
      "Topic 16: music tour opera singer israel people film\n",
      "Topic 17: church catholic bernardin cardinal bishop wright death\n",
      "Topic 18: harriman clinton u.s ambassador paris president churchill\n",
      "Topic 19: city museum art exhibition century million churches\n"
     ]
    }
   ],
   "source": [
    "#官网文档：https://lda.readthedocs.io/en/latest/getting_started.html\n",
    "import numpy as np\n",
    "import lda\n",
    "#需要将datasets更换为自定义的数据集\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "X.shape\n",
    "X.sum()\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、利用LDA分析影评语料的话题组成，输出前10个话题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "all_words=all_words.most_common(2000) #词频表按频率排序\n",
    "badwords=nltk.corpus.stopwords.words('english')\t#去停止词\n",
    "word_features  =[w for (w,f) in all_words if w not in badwords]\n",
    "features = np.zeros([len(documents),len(word_features)],dtype=float)\n",
    "for n in range(len(documents)):\n",
    "    document_words = set(documents[n][0])\n",
    "    for  m in range(len(word_features)):\n",
    "        if word_features[m] in document_words:\n",
    "            features[n,m] = 1\n",
    "target=[c for (d,c) in documents]\n",
    "\n",
    "train_set=features[:1000,:]\n",
    "target_train=target[:1000]\n",
    "test_set=features[1000:,:]\n",
    "target_test=target[1000:]\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-cc6e9e622206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# 构造词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# 基于词典，使【词】→【稀疏向量】，并将向量放入列表，形成【稀疏向量集】\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.6\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.6\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         logger.info(\n",
      "\u001b[1;32md:\\python3.6\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import jieba.posseg as jp, jieba\n",
    "import numpy as np\n",
    "import lda\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "import string\n",
    "#需要将datasets更换为自定义的数据集\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "all_words=all_words.most_common(2000) #词频表按频率排序\n",
    "badwords=nltk.corpus.stopwords.words('english')\t#去停止词\n",
    "word_features  =[w for (w,f) in all_words if w not in badwords]\n",
    "texts=[]\n",
    "for word in word_features:\n",
    "    texts.append(word)\n",
    "    \n",
    "# 构造词典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# 基于词典，使【词】→【稀疏向量】，并将向量放入列表，形成【稀疏向量集】\n",
    "corpus = [dictionary.doc2bow(words) for words in texts]\n",
    "\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# 主题推断\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、利用LDA分析Sogou语料的话题组成，输出前10个话题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(721 unique tokens: ['CEO', 'DCM', 'Paypal', 'VISA', 'VISA卡']...) from 5 documents (total 1393 corpus positions)\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.5\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.5\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 2 topics, 1 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-7.256 per-word bound, 152.9 perplexity estimate based on a held-out corpus of 5 documents with 1393 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #5/5\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.500): 0.027*\"支付\" + 0.021*\"公司\" + 0.014*\"快\" + 0.014*\"钱\" + 0.011*\"用户\" + 0.008*\"网络\" + 0.008*\"上海\" + 0.007*\"中国\" + 0.007*\"市场\" + 0.006*\"消费者\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.500): 0.013*\"支付\" + 0.013*\"快\" + 0.012*\"钱\" + 0.011*\"公司\" + 0.010*\"消费者\" + 0.008*\"上海\" + 0.007*\"关国光\" + 0.006*\"中国\" + 0.006*\"交易\" + 0.006*\"网络\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.569744, rho=1.000000\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.500): 0.027*\"支付\" + 0.021*\"公司\" + 0.014*\"快\" + 0.014*\"钱\" + 0.011*\"用户\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.500): 0.013*\"支付\" + 0.013*\"快\" + 0.012*\"钱\" + 0.011*\"公司\" + 0.010*\"消费者\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.027*\"支付\" + 0.021*\"公司\" + 0.014*\"快\" + 0.014*\"钱\" + 0.011*\"用户\"')\n",
      "(1, '0.013*\"支付\" + 0.013*\"快\" + 0.012*\"钱\" + 0.011*\"公司\" + 0.010*\"消费者\"')\n",
      "(array([[621.04047   ,   1.8926458 ],\n",
      "       [ 79.692604  ,  13.290045  ],\n",
      "       [  1.0402694 , 263.93134   ],\n",
      "       [204.08127   ,   0.89753544],\n",
      "       [211.34174   ,   0.63713783]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import jieba.posseg as jp, jieba\n",
    "import numpy as np\n",
    "import lda\n",
    "#需要将datasets更换为自定义的数据集\n",
    "#X = lda.datasets.load_reuters()\n",
    "#vocab = lda.datasets.load_reuters_vocab()\n",
    "#titles = lda.datasets.load_reuters_titles()\n",
    "#X.shape\n",
    "#X.sum()\n",
    "# 文本集\n",
    "\n",
    "\n",
    "#构建停用词表\n",
    "#停用词表来源：https://github.com/goto456/stopwords\n",
    "stop_words = r'C:/Users/28347/Desktop/NLP/stopwords-master/hit_stopwords.txt'\n",
    "stopwords = codecs.open(stop_words,'r',encoding='utf8').readlines()\n",
    "stopwords = [ w.strip() for w in stopwords ]\n",
    "\n",
    "#结巴分词后的停用词性 [标点符号、连词、助词、副词、介词、时语素、‘的’、数词、方位词、代词]\n",
    "\n",
    "stop_flag = ['x', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']\n",
    "\n",
    "#对一篇文章分词、去停用词\n",
    "\n",
    "def tokenization(filename):\n",
    "    result = []\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        words = pseg.cut(text)\n",
    "    for word, flag in words:\n",
    "        if flag not in stop_flag and word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "filenames = ['C:/Users/28347/Desktop/NLP/Sogou/C000008/10.txt', \n",
    "             'C:/Users/28347/Desktop/NLP/Sogou/C000008/11.txt',\n",
    "             'C:/Users/28347/Desktop/NLP/Sogou/C000008/12.txt',\n",
    "             'C:/Users/28347/Desktop/NLP/Sogou/C000008/13.txt',\n",
    "             'C:/Users/28347/Desktop/NLP/Sogou/C000008/14.txt'\n",
    "            ]\n",
    "texts = []\n",
    "for each in filenames:\n",
    "    texts.append(tokenization(each))\n",
    "    \n",
    "# 构造词典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# 基于词典，使【词】→【稀疏向量】，并将向量放入列表，形成【稀疏向量集】\n",
    "corpus = [dictionary.doc2bow(words) for words in texts]\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# 主题推断\n",
    "print(lda.inference(corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
